{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classification example ( movie review )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "from keras.datasets import imdb\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_data),  len(train_labels), len(test_data), len(test_labels))\n",
    "print()\n",
    "print(\"max length of sequance: \", max([len(seq) for seq in train_data] ) )\n",
    "print()\n",
    "print(\"train data sampling:\\n\", train_data[0])\n",
    "print()\n",
    "print(\"label:\", train_labels)\n",
    "print()\n",
    "\n",
    "# check max index of words\n",
    "max([max(seq) for seq in train_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert to original words (just test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "word_index\n",
    "\n",
    "reverse_word_index = dict( [(value, key) for (key, value) in word_index.items() ] )\n",
    "decoded_review = ' '.join( [reverse_word_index.get(i-3, '?') for i in train_data[0]] )\n",
    "decoded_review\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reday for data\n",
    "- we cant inject numeric list to NN. so need to change list to tensor.\n",
    "- there are two ways to make change.\n",
    "- first method is `Embedding`\n",
    "- second method is `One-hot encoding`\n",
    "- in this section, we will use One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    # 크기가 (len(sequences), dimension))이고 모든 원소가 0인 행렬을 만듭니다\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.  # results[i]에서 특정 인덱스의 위치를 1로 만듭니다        \n",
    "    return results\n",
    "\n",
    "# 훈련 데이터를 벡터로 변환합니다\n",
    "x_train = vectorize_sequences(train_data)\n",
    "# 테스트 데이터를 벡터로 변환합니다\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize label\n",
    "\n",
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num = len(train_data)\n",
    "test_num = len(train_data)\n",
    "\n",
    "train_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000, )) )\n",
    "model.add(layers.Dense(16, activation='relu') )\n",
    "model.add(layers.Dense(1, activation='sigmoid') )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prepare validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train[:10000]\n",
    "partial_x_train = x_train[10000:]\n",
    "y_val = y_train[:10000]\n",
    "partial_y_train = y_train[10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "network compile\n",
    "- optimizer나 loss, metrics 등을 custom 가능함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import losses\n",
    "from keras import metrics\n",
    "\n",
    "# use function\n",
    "model.compile(optimizer=optimizers.RMSprop(),\n",
    "                loss=losses.binary_crossentropy,\n",
    "                metrics=[metrics.binary_accuracy]\n",
    "               )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# use string\n",
    "model.compile(optimizer='rmsprop',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy']\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(partial_x_train, partial_y_train, \n",
    "                   batch_size=512, \n",
    "                   epochs=20,\n",
    "                   validation_data=(x_val, y_val)\n",
    "                  )\n",
    "\n",
    "result = model.evaluate(x_test, y_test)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check `history` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = hist.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "draw cost graph (train, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = hist.history['binary_accuracy']\n",
    "val_acc = hist.history['val_binary_accuracy']\n",
    "loss = hist.history['loss']\n",
    "val_loss = hist.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# ‘bo’는 파란색 점을 의미합니다\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# ‘b’는 파란색 실선을 의미합니다\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "\n",
    "plt.plot(epochs, acc, 'ro', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try make model again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras import *\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(32, activation='relu', input_shape=(10000, )) )\n",
    "model.add(layers.Dense(32, activation='relu') )\n",
    "model.add(layers.Dense(1, activation=activations.sigmoid) )\n",
    "\n",
    "# use function\n",
    "model.compile(optimizer=optimizers.RMSprop(),\n",
    "                loss=losses.binary_crossentropy,\n",
    "#                 loss=losses.mse,\n",
    "                metrics=[metrics.binary_accuracy]\n",
    "               )\n",
    "\n",
    "# learning\n",
    "tb_hist = callbacks.TensorBoard(log_dir='./graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "\n",
    "hist = model.fit(partial_x_train, partial_y_train, \n",
    "                    batch_size=512, \n",
    "                    epochs=20,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                     callbacks=[tb_hist]                 \n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.evaluate(x_test, y_test)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensorboard 연동"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash --bg -s\n",
    "nohup tensorboard --logdir=\"/home/boom109/workspace/keras-books-notebooks/soonhyung_notebooks/graph\" 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser\n",
    "webbrowser.open(\"http://0.0.0.0:6006\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#실습\n",
    "from keras import activations\n",
    "from keras import regularizers\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "m = models.Sequential()\n",
    "m.add(layers.Dense(100, activation=\"relu\", input_shape=(10000, )))\n",
    "#m.add(layers.Dropout(0.5))\n",
    "m.add(layers.Dense(50, activation=\"relu\", kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.01)))\n",
    "m.add(layers.Dense(25, activation=\"relu\", kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.01)))\n",
    "m.add(layers.Dense(1, activation=activations.tanh))\n",
    "m.summary()      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "from keras import losses\n",
    "from keras import metrics\n",
    "m.compile(optimizer=optimizers.RMSprop(lr=0.001), loss=losses.binary_crossentropy, metrics=[metrics.binary_accuracy])\n",
    "m.fit(partial_x_train, partial_y_train, batch_size=512, epochs=30, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash --bg -s\n",
    "nohup tensorboard --logdir=\"/tf/notebooks/zo/soonhyung_notebooks/graph\" 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser\n",
    "webbrowser.open(\"http://0.0.0.0:6006\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({\"predict\":m.predict(x_test), \"label\":y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
